{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":95812,"sourceType":"datasetVersion","datasetId":504}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.feature_selection import SelectKBest, chi2\nimport missingno as msno\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Carga de datos\ngtd_data = pd.read_csv(\"/kaggle/input/gtd\")\nprint(gtd_data.info())\n\n# ------------- LIMPIEZA PREVIA A LA SELECCIÓN -------------\n\n# Convertimos a NaN los espacios en blanco:\ngtd_data_limpieza = gtd_data.replace([\"\", \" \", \"Unknown\", -9, -99], np.nan)\nprint(gtd_data_limpieza.info())\n\n# Quitamos columnas con terminación _txt\ncolumnas_txt = gtd_data_limpieza.columns[gtd_data_limpieza.columns.str.endswith('_txt')]\nprint(columnas_txt)\n\ndef quitar_terminacion_txt(columna_txt):\n    return columna_txt.replace('_txt', '')\n\n# Crear una función para asociar las filas únicas y llenar valores faltantes\ndef crear_asociacion_unica_y_llenar(gtd_data, variable1, variable2):\n    asociacion_unica = {}\n    gtd_data[variable2] = gtd_data[variable2].astype(str).replace([\"\", \" \"], np.nan)\n\n    for i in range(len(gtd_data)):\n        if pd.notna(gtd_data.loc[i, variable1]) and pd.notna(gtd_data.loc[i, variable2]):\n            asociacion_unica[gtd_data.loc[i, variable1]] = gtd_data.loc[i, variable2]\n\n    for i in range(len(gtd_data)):\n        if pd.isna(gtd_data.loc[i, variable1]) and pd.notna(gtd_data.loc[i, variable2]):\n            gtd_data.loc[i, variable1] = next((key for key, value in asociacion_unica.items() if value == gtd_data.loc[i, variable2]), np.nan)\n    \n    return gtd_data\n\n# Recorrer las columnas que terminan en \"_txt\"\nfor columna in columnas_txt:\n    col_sin_txt = quitar_terminacion_txt(columna)\n    gtd_data_limpieza = crear_asociacion_unica_y_llenar(gtd_data_limpieza, col_sin_txt, columna)\n\n# Eliminar columnas con terminación en _txt\ngtd_data_limpieza.drop(columns=columnas_txt, inplace=True)\n\nprint(gtd_data_limpieza.info())\n\n# -- Eliminación de columnas con más del 90% de valores perdidos: --\n\ndef eliminar_columnas_valores_perdidos(datos, umbral=90):\n    porcentaje_perdido = datos.isna().mean() * 100\n    columnas_a_eliminar = porcentaje_perdido[porcentaje_perdido > umbral].index\n    return datos.drop(columns=columnas_a_eliminar)\n\ngtd_data_limpieza = eliminar_columnas_valores_perdidos(gtd_data_limpieza, 90)\nprint(gtd_data_limpieza.info())\n\n# Eliminamos columnas de forma manual\ncolumnas_a_eliminar = [\"location\", \"summary\", \"nwound\", \"propcomment\", \"addnotes\", \"scite2\", \"scite3\", \"related\"]\ngtd_data_limpieza.drop(columns=columnas_a_eliminar, inplace=True)\nprint(gtd_data_limpieza.info())\n\n# ---- IGUALAMOS LA CANTIDAD DE INSTANCIAS DE success = 0 CON las = a 1 ---- \ngtd_data_s1 = gtd_data_limpieza[gtd_data_limpieza['success'] == 1]\ngtd_data_s0 = gtd_data_limpieza[gtd_data_limpieza['success'] == 0]\noccurrences_s0 = len(gtd_data_s0)\ngtd_data_s1 = gtd_data_s1.sample(n=occurrences_s0, random_state=123)\nmuestra = pd.concat([gtd_data_s1, gtd_data_s0], ignore_index=True)\n\n# ---- SELECCIÓN DE ATRIBUTOS CON CHI^2 -----\nX = muestra.drop(columns='success')\ny = muestra['success']\n\n# Usando chi-squared para seleccionar las mejores características\nchi2_selector = SelectKBest(chi2, k='all')\nX_chi2 = chi2_selector.fit_transform(X.select_dtypes(include=[np.number]), y)\n\n# Crear un DataFrame con las importancias\npesos = pd.DataFrame(chi2_selector.scores_, index=X.select_dtypes(include=[np.number]).columns, columns=['attr_importance'])\norden = pesos['attr_importance'].sort_values()\npesos = pesos[pesos['attr_importance'] > 0]\n\n# Visualizar la importancia\nplt.figure(figsize=(10, 6))\nplt.barh(orden.index, orden.values)\nplt.xlabel(\"Importancia\")\nplt.title(\"Importancia de Atributos - Chi²\")\nplt.show()\n\n# Crea muestra con las variables que nos sirven\ngtd_data_seleccion = muestra[pesos.index]\ngtd_data_seleccion.insert(0, 'eventid', muestra['eventid'])\ngtd_data_seleccion['success'] = muestra['success']\n\nprint(gtd_data_seleccion.info())\n\n# ---- TRATAMIENTO DE VALORES PERDIDOS -----\n\n# Reemplazando por media / moda / mediana\ndef mode2(x):\n    return x.mode()[0]\n\ndef imputacion(data):\n    for var in data.columns:\n        if data[var].dtype == 'float64':\n            data[var].fillna(data[var].mean(), inplace=True)\n        elif data[var].dtype == 'object':\n            no_empty = data[var].dropna().replace(\"\", np.nan).dropna()\n            m = mode2(no_empty)\n            data[var].fillna(m, inplace=True)\n            data[var].replace(\"\", m, inplace=True)\n        elif data[var].dtype == 'int64':\n            data[var].fillna(data[var].median(), inplace=True)\n    return data\n\ngtd_data_sin_vp = imputacion(gtd_data_seleccion)\nprint(gtd_data_sin_vp.info())\n\n# Reemplazando usando aprendizaje automático:\nfrom missingpy import MissForest\n\n# Usar un muestreo del 1%\nporcentaje_muestreo = 0.01\ntamano_muestra = int(len(muestra) * porcentaje_muestreo)\nmuestra = muestra.sample(n=tamano_muestra, random_state=1)\nprint(muestra.info())\n\n# Imputar los valores perdidos usando MissForest\nimputador = MissForest()\ngtd_data_sin_vp2 = imputador.fit_transform(muestra.select_dtypes(include=[np.number]))\ngtd_data_sin_vp2 = pd.DataFrame(gtd_data_sin_vp2, columns=muestra.select_dtypes(include=[np.number]).columns)\n\n# ---- ELIMINACIÓN DE VALORES ATÍPICOS -----\ncolumnas_con_nas = gtd_data_sin_vp.columns[gtd_data_sin_vp.isna().any()].tolist()\nprint(columnas_con_nas)\n\ndef quitar_atipicos(columna, threshold=1.5):\n    iqr = np.percentile(columna, 75) - np.percentile(columna, 25)\n    lower_limit = np.percentile(columna, 50) - threshold * iqr\n    upper_limit = np.percentile(columna, 50) + threshold * iqr\n    return columna.mask((columna < lower_limit) | (columna > upper_limit))\n\nfor nombre_columna in gtd_data_sin_vp.select_dtypes(include=[np.number]).columns:\n    gtd_data_sin_vp[nombre_columna] = quitar_atipicos(gtd_data_sin_vp[nombre_columna])\n\nprint(gtd_data_sin_vp.head())\nprint(gtd_data_sin_vp.info())\n\nhay_nas_en_data = gtd_data_sin_vp.isna().any().any()\nprint(hay_nas_en_data)\n\n# ---- DISCRETIZACIÓN -----\ndef discretizar_por_rango(data):\n    columnas_numericas = data.select_dtypes(include=[np.number]).columns\n    for columna in columnas_numericas:\n        print(f\"Rango de {columna}: {data[columna].min()} - {data[columna].max()}\")\n        num_bins = int(np.ceil(np.log2(len(data[columna])) + 1))\n        data[columna] = pd.cut(data[columna], bins=num_bins)\n    return data\n\ngtd_data_discretizado = discretizar_por_rango(gtd_data_sin_vp.copy())\nprint(gtd_data_discretizado.head())\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}